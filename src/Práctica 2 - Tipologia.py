# -*- coding: utf-8 -*-
"""Práctica 2 - Tipologia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bxg7nJyrudBRuMD-4oE33xCGrPNqEcmb

# **PRÁCTICA 2**

En primer lugar, como es común en Jupyter Notebook, se procede a importar todas las librerias y que se usarán durante la práctica
"""

import pandas
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import norm, shapiro, mannwhitneyu
from sklearn import preprocessing, metrics
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from scipy import stats
import re
import statistics as st

"""# **1. Descripción del dataset**

## **Carga del DataSet**

A continuación, procedemos a cargar el dataset extraído de Kaggle, del link: https://www.kaggle.com/ionaskel/laptop-prices. Se ha creado un enlace al link de la práctica para proceder a la carga del archivo.
"""

# Se carga el archivo con referencia al gitHub donde se encuentra el mismo
laptops_file = 'https://raw.githubusercontent.com/carlosalloUOC/PRA2-Limpieza-Analisis/main/csv/laptops_inicial.csv'

# Se lee el fichero anterior, para transformarlo en un dataframe indicándole que en la primera línea se encuentran las cabecera
laptops_initial = pandas.read_csv(laptops_file, header=0, encoding='latin-1')

# Comprobamos que tiene las dimensiones correctas
print(laptops_initial.shape)

"""Se aprecia como se ha importado un dataset de 1303 filas y 13 columnas. Se puede comprabar que este hecho es correcto, ya que en la web mencionada donde se han descargado los datos se proporciona esta información. A continuación, se procede a imprimir las 5 primeras filas del dataframe, para entender el tipo de datos con el que se va a trabajar:"""

laptops_initial.head(5)

"""## **Descripción de variables**

Se aprecia que se cuentan con las siguientes columnas:
* Unnamed: 0	
* Company	
* Product	
* TypeName	
* Inches	
* ScreenResolution	
* Cpu	
* Ram	
* Memory	
* Gpu	
* OpSys	
* Weight	
* Price_euros

Aunque su nombre es bastante descriptivo, en la documentación se muestra la siguente información sobre estas columnas:
* Unnamed: 0	--> No da información, pero por su estructura en los datos, se aprecia que tiene tiene la mera función de ID_descendente para cada una de las columnas
* Company	--> Company Name
* Product	--> Product Name
* TypeName	--> Laptop Type
* Inches	--> Screen Inches
* ScreenResolution	--> Screen Resolution
* Cpu	--> CPU Model
* Ram	--> RAM Characteristics
* Memory	--> Memory
* Gpu	--> GPU Characteristics
* OpSys	--> Operating System
* Weight	--> Laptop's Weight
* Price_euros --> Laptop's Price

Además se aprecia que, para valores numéricos, tan solo se presenta sin unidades de medida el caso del precio (donde se indica que son euros en el nombre de la variable (Price_euros)), al igual que ocurre con el tamaño que ya se da en pulgadas. Para otros casos, como el caso del peso, la memoria o la RAM, no ocurre este hecho al proporcionarse la unidad de medida junto con el valor. Esto, como se verá en pasos posteriores, será objeto de tratamiento para poder hacer de esta manera el estudio de homegeneidad, realizar representaciones o por si se quieren construir modelos a partir de los datos.

En función del perfil del comprador, el desconocimiento de la gama de productos y sus características puede que los lleve a tomar una decisión de compra poco adecuada o ajustada en precio. Por este motivo, sería interesante conocer qué variables o características son más influyentes en el precio a la hora de compra de un ordenador, para que de esta manera, el comprador:
* Pueda hacerse una idea del presupuesto aproximado que tendrá el ordenador que desea en base a las características técnicas deseadas.
* Pueda conocer si es verdad que algunas marcas, como Apple, tienen un precio algo superior al resto de las marcas.
* Pueda conocer qué características son las más influyentes en el precio para en base a estas, poder centrarse en lo que realmente necesita para incrementar o decrementar su presupuesto.

# **2. Integración y selección**

En esta sección, los datos con los que se van a trabajar vienen todos en un mismo dataSet, por lo que no requiere ninguna integración adicional a la importación realizada. Adicionalmente, cabe decir que si se quisieran usar dos dataSet diferentes con las mismas características, se debería de realizar una integración de los mismos en donde habría que tener en cuenta posibles repeticiones de objetos, que las propiedades se presenten en las mismas unidades... Este proceso se realizó en la práctica 1, en el momento en el que se integraban dos dataSets diferentes (uno de cada web en donde se realizó WebScraping), en uno sólo.

Respecto a la selección de los datos, nos quedaremos con todas las filas, ya que cada una de ellas corresponde a un ordenador diferente y aporta información al estudio que se está realizando. Ademas, nos encontramos ante un número de ordenadores (1003) no tan grande como para tener que hacer reducción de la cantidad. Sin embargo, para el caso de tener que realizar el mismo, consideramos que las dos mejores formas de hacerlo serían el método de muestra aleatoria simple sin sustitución (para no tener repeticiones de ordenadores en el dataSet resultante), o muestra de clústeres, en donde cada cluster podría estar correspondido por la marca o por intervalos de precio para asegurar que tenemos muestras de todos los precios. 

Si que en este apartado sería interesante hacer una división de los datos en muestra y test para por si posteriormente se usaran datos para realizar modelos. Sin embargo, este paso se realizará más adelante cuando se haya hecho el proceso de limpieza completo de los datos, ya que sino habría que realizar el proceso dos veces. 

Sobre las columnas, encontramos la primera de ellas que no nos da ninguna información útil para el estudio y ser un simple id ascendente, con lo cual la eliminaremos. Haremos lo mismo con la columna que se refiere al producto, pues el objetivo en todo momento es realizar una comparación en base a características o marcas, pero no en base al modelo del portátil directamente.
"""

# Eliminamos la columna Unnamed: 0
del laptops_initial["Unnamed: 0"]

# Eliminamos la columna Product
del laptops_initial["Product"]

#Imprimimos los datos para comprobar la correcta eliminación
laptops_initial.head(5)

"""# **3. Limpieza de los datos**

## **Elementos vacíos I**

En este apartado se prestará atención a los valores que se presentan como nulos o incluso a los que, sin ser nulos, presentan valores anormales que indicarían que nos encontramos ante un caso de elemento vacío o perdido (por ejemplo, el hecho de que en pulgadas se encuentre un 0)

Se comenzará con los valores nulos del dataFrame. Para ello, en primer lugar se hará un resumen de valores nulos por columna:
"""

laptops_initial.isnull().sum()

"""Vemos como no hay ningún valor nulo. Para datasets con más columnas es más útil ver si hay algún valor nulo por medio del booleano, para que si este resulta verdadero, mirar dónde está el mismo"""

laptops_initial.isnull().values.any()

"""Aunque no se han encontrado datos vacíos, se ha de prestar atención a que no se presenten en forma de string del tipo "No available". Esto se realizará en el apartado "Elementos vacíos II", una vez los datos hayan sido sido preparados para el estudio

## **Preparación de datos**

Es interesante realizar una limpieza de las variables para su posterior tratamiento.

En primer lugar, centraremos la atención para que las variables que puedan ser consideradas como numéricas (Inches, Ram, Weight o Price_euros), sean tratadas así. Para el caso de Inches o Price_euros esto ya ocurre, pero para Ram o Weight no ocurre al aparecer sus unidades de medida como son GB o Kg. 

Aunque parece que en el dataSet todas las medidas están dadas en GB y Kg, lo primero que se realizará será ver si para todas las columnas se cumple este hecho. Comenzamos por la Ram:
"""

#Se realiza un splitado en base a 'GB'
split_ram_value = laptops_initial['Ram'].str.split('GB', 0, expand=True)
#Se comprueba que todos los valores son numericos (no lo serían si hubiera otra medida)
split_ram_value[0].str.isnumeric().unique()

"""Se aprecia que todos los valores que se encuentran en la variable RAM vienen dados en GB, ya que al realizar el splitado se ha comprobado que en la primera posición del resultado se encuentran todo numeros enteros, por lo que será usaran estos valores numéricos posteriormente (almacenados en split_ram_value[0]).

De forma similar, se realizará el mismo estudio para la columna peso, peso con su unidad de Kg 

"""

# Se realiza un splitado en base a 'kg'
split_weight_value = laptops_initial['Weight'].str.split('kg', 0, expand=True)

# Se comprueba que todos los valores son enteros como en el caso anterior o float. Para ello, se define la función is_valid_decimal que
# indicará si un numero es dicimal (True) o no lo es:
def is_valid_decimal(string):
    try:
        float(string)
        return True
    except ValueError:
        return False

# Se crea un objeto serie para almacenar los resultados
is_weight_decimal = pandas.Series([])

# Se recorre la serie con los posibles números resultantes del splitado, y se van añadiendo a la serie creada
for index, value in split_weight_value[0].items():
    is_weight_decimal[index] = is_valid_decimal(value)
    
# Vemos los diferentes valores que contiene le vector (si todos son true, todos serán valores float por lo que todos serán kg)
is_weight_decimal.unique()

"""Por lo tanto, una vez se ha comprobado que ambas columnas eliminando la medida tiene un valor numérico, se procede a transformar estas columnas a numericas. Para ello, la unidad de medida, al ser en todas ellas la misma se incluirá en la columna a la que corresponde:"""

# Se renombran las columnas, añadiendo los GB a la RAM y los KG al peso
laptops_initial = laptops_initial.rename(columns={'Ram': 'Ram(GB)', 'Weight': 'Weight(Kg)'})

# Se hace el camnio en estas columnas por los números extraídos
laptops_initial["Ram(GB)"] = split_ram_value[0]
laptops_initial["Weight(Kg)"] = split_weight_value[0]

# Se imprime la cabecera
laptops_initial.head(5)

"""Una vez se tienen estos datos correctamente transformados, se ponen de tipo numérico las columnas mencionadas"""

# Transformamos esta columnas al tipo numérico
laptops_initial["Ram(GB)"] = pandas.to_numeric(laptops_initial["Ram(GB)"])
laptops_initial["Weight(Kg)"] = pandas.to_numeric(laptops_initial["Weight(Kg)"])
laptops_initial["Inches"] = pandas.to_numeric(laptops_initial["Inches"])
laptops_initial["Price_euros"] = pandas.to_numeric(laptops_initial["Price_euros"])
laptops_initial.head(5)

"""Prestando atención a la variable Memoria, vemos que hay dos partes bastante diferenciadas. Aparentemente, en un primer lugar encontramos la unidad de memoria (que sería un caso parecido al atratado con la RAM), y por otro, el tipo de memoria. Por lo tanto, en un primer lugar se analizarán los tipos de memoria de los que disponemos, haciendo el splitado en base al primer espacio:"""

# Se realiza un splitado en base tan solo al primer espacio , ya que es lo aparentemente separa la cantidad de memoria y el tipo
split_memory_value = laptops_initial['Memory'].str.split(' ', 1, expand=True)

# Imprimimos las segundas partes del splitado
split_memory_value[1].unique()

"""Vemos que nos encontramos ante un problema imprevisto. Al ser tantos datos, se acaba de comprobar que la estructura de cantidad de almacenamiento + tipo no siempre es así, pudiendo a veces combinarse combinaciones de varios tipos de memoria. Además, también con respecto a las unidades, parece que no todas son GB como en el caso anterior. Encontramos en este punto unidades tanto en GB como en TB. 

Haremos un primer estudio sobre a cuántos datos afecta este hecho de estar formado por más de un tipo de memoria.  
"""

laptops_initial[laptops_initial['Memory'].str.contains(' + ')]

"""Contamos con 4 tipos de memoria: 
* SSD
* Flash Storage
* HDD
* Hybrid

Que se presentan de forma única o con las siguientes combinaciones:
* SSD +  SSD
* HDD +  HDD
* SSD +  HDD
* Flash Storage +  HDD
* SSD +  Hybrid

Teniendo estas en cuenta, se elaborará una serie para cada una de ellas, con el fin de tener controlado tanto el tipo de memoria como su valor. Se había pensado también en juntar los valores de ambas memorias en tan solo una columna, indicando en el tipo de memoria el conjunto de memorias a las que pertenece. Sin embargo, al realizar esto, se perdería información relevante, ya que no es lo mismo una memoria SSD y una HDD!

Adicionalmente, se escogerá la unidad de medida de GB, ya que es la que más presente está. Se deberá de tener en cuenta que 1TB serán 1024GB, ya que son las dos medidas que aparecen.
"""

# Se crea un objeto serie para almacenar cada tipo de memoria
ssd_value = pandas.Series([])
flash_value = pandas.Series([])
hdd_value = pandas.Series([])
hybrid_value = pandas.Series([])

# Devuelve los GB a partir de la unidad de memoria proporcionada 
def convert_GB(memoryNumber):
  if "GB" in memoryNumber:
    return (float(memoryNumber.split('GB')[0]))
  else:
    return (float(memoryNumber.split('TB')[0])*1024)

# Devuelve el tipo de memoria principal (la que aparece en primer lugar)
def get_primary_type(secondaryMemory):
  secondaryMemory_split_plus = secondaryMemory.split(' +  ')
  return secondaryMemory_split_plus[0]

# Devuelve el tipo de memoria secundaria (la que aparece en segundo lugar)
def get_secondary_type(secondaryMemory):
  secondaryMemory_split_plus = secondaryMemory.split(' +  ')
  secondaryMemory_split_value =secondaryMemory_split_plus[1].split(' ')
  return secondaryMemory_split_value[1]

# Devuelve el valor de memoria secundaria (la que aparece en segundo lugar)
def get_secondary_value(secondaryMemory):
  secondaryMemory_split_plus = secondaryMemory.split(' +  ')
  secondaryMemory_split_value =secondaryMemory_split_plus[1].split(' ')
  return secondaryMemory_split_value[0]


# Se recorre la serie con los posibles números resultantes del splitado, y se van añadiendo a la serie creada
for index, value in split_memory_value[0].items():
  # El primer caso será el que incluya un + en el string, hecho que indica que esta compuesto por dos tipos de memoria
  if " + " in split_memory_value[1][index]:
    # Se extrae el valor y el tipo secundario 
    primary_type = get_primary_type(split_memory_value[1][index])
    secondary_value = get_secondary_value(split_memory_value[1][index])
    secondary_type = get_secondary_type(split_memory_value[1][index])
    # Se comprueban que si ambos son del mismo tipo (ocurre con SSD o HDD), en cuyo caso habrá que sumar estos valores
    if (primary_type == secondary_type):
      if (primary_type == 'HDD'):
        hdd_value[index] = convert_GB(split_memory_value[0][index]) + convert_GB(secondary_value)
        ssd_value[index] = 0
        hybrid_value[index] = 0
        flash_value[index] = 0
      else:
        ssd_value[index] = convert_GB(split_memory_value[0][index]) + convert_GB(secondary_value)
        hdd_value[index] = 0
        hybrid_value[index] = 0
        flash_value[index] = 0
    # En el caso de que no sean igual, se trata de dos tipos de memorias diferentes, en donde teniendo en cuentas las combinaciones posibles en base a las combinaciones vistas anteriormente
    else:
      if (primary_type == 'SSD' and secondary_type == 'HDD'):
        ssd_value[index] = convert_GB(split_memory_value[0][index])
        hdd_value[index] = convert_GB(secondary_value)
        hybrid_value[index] = 0
        flash_value[index] = 0
      elif (primary_type == 'Flash Storage' and secondary_type == 'HDD'):
        ssd_value[index] = 0
        hdd_value[index] = convert_GB(secondary_value)
        hybrid_value[index] = 0
        flash_value[index] = convert_GB(split_memory_value[0][index])
      elif (primary_type == 'SSD' and secondary_type == 'Hybrid'):
        ssd_value[index] = convert_GB(split_memory_value[0][index])
        hdd_value[index] = 0
        hybrid_value[index] = convert_GB(secondary_value)
        flash_value[index] = 0

  # En el caso de que no se encuentre la particula +, será porque sólo tiene un tipo de memoria, por lo que buscaremos cual es y lo introduciremos a la serie, dejando el resto de valores
  # de la memoria que no corresponda a 0
  else:
    # Si es SSD, rellenamos la serie de SSD, mientras que en las demás introdimos un 0, al no contar con esa memoria
    if split_memory_value[1][index] == 'SSD':
      ssd_value[index] = convert_GB(split_memory_value[0][index])
      hdd_value[index] = 0
      hybrid_value[index] = 0
      flash_value[index] = 0
    elif split_memory_value[1][index] == 'HDD':
      hdd_value[index] = convert_GB(split_memory_value[0][index])
      ssd_value[index] = 0
      hybrid_value[index] = 0
      flash_value[index] = 0
    elif split_memory_value[1][index] == 'Hybrid':
      hybrid_value[index] = convert_GB(split_memory_value[0][index])
      ssd_value[index] = 0
      hdd_value[index] = 0
      flash_value[index] = 0
    elif split_memory_value[1][index] == 'Flash Storage':
      flash_value[index] = convert_GB(split_memory_value[0][index])
      ssd_value[index] = 0
      hdd_value[index] = 0
      hybrid_value[index] = 0

"""Para comprobar que se ha hecho bien el cambio, se imprimirá uno de los casos especiales (que incluyen dos tipos de memoria), y que se ha visto en el printeo de los mismos. Este será la muestra 21, que tiene segun vemos en la tabla: 128GB SSD + 1TB HDD	"""

print('Valor SSD en GB: ' + str(ssd_value[21]))
print('Valor HDD en GB: ' + str(hdd_value[21]))

"""Una vez hecho este proceso, se colocarán las 4 nuevas columnas con cada tipo de memoria, siendo este un valor numérico:"""

# Introducimos las nuevas columnas
laptops_initial["MemorySSD(GB)"] = ssd_value
laptops_initial["MemoryHDD(GB)"] = hdd_value
laptops_initial["MemoryFlash(GB)"] = flash_value
laptops_initial["MemoryHybrid(GB)"] = hybrid_value

# Las transformamos a numéricas
laptops_initial["MemorySSD(GB)"] = pandas.to_numeric(laptops_initial["MemorySSD(GB)"])
laptops_initial["MemoryHDD(GB)"] = pandas.to_numeric(laptops_initial["MemoryHDD(GB)"])
laptops_initial["MemoryFlash(GB)"] = pandas.to_numeric(laptops_initial["MemoryFlash(GB)"])
laptops_initial["MemoryHybrid(GB)"] = pandas.to_numeric(laptops_initial["MemoryHybrid(GB)"])

# Eliminamos la columna inicial que tenia los datos de la memoria sin limpiar
del laptops_initial["Memory"]

# Verificamos la correcta creación
laptops_initial.head(5)

"""Se procederá ahora a realizar una sepación de la CPU. Aparentemente, parece que en una primera partaparece la empresa que la proporciona, en segundo la versión que se proporciona, y por último, la velocidad del mismo. Como siempre, se comprueba gracias a la programación que para todos ocurre esto (ya que al contar con 1303 muestras, no nos podemos fijar tan solo en las 5 primeras)

"""

# Se realiza un splitado en base al primer espacio, dejando en la primera parte la supuesta marca, mientras que en la segunda el resto
split_cpu_company_value = laptops_initial['Cpu'].str.split(' ', 1, expand=True)

# Almacenamos las marcas de cpu
company_cpu = split_cpu_company_value[0]

# Comprobamos que efectivamente todas ellas corresponden a marcas
company_cpu.unique()

"""Se aprecia como en esta ocasión, se está en lo cierto y tan solo hay 3 marcas. Procederemos ahora a ver los tipos de productos trabajando con la parte del splitado anterior no usada"""

# Se realiza un splitado de la segunda parte, en donde el mismo se realizará en base al último espacio, al ser la estructura VERSION + ' ' + VELOCIDAD
split_cpu_version_and_speed_value = split_cpu_company_value[1].str.rsplit(' ', 1, expand=True)

# Sacamos las versiones, que estarán en la primera parte del splitado
version_cpu=split_cpu_version_and_speed_value[0]

# Comprobamos que todas ellas son versiones
version_cpu.unique()

"""En este caso tenemos más versiones que marcas, pero vemos como todas ellas son versiones de los diferentes proveedores, por lo que tan solo quedará comprobar que, efectivamente, el final de este string coincide en todos casos con velocidades"""

# Sacamos las velocidades, que estarán en la seguna parte del splitado anterior
speed_cpu=split_cpu_version_and_speed_value[1]
speed_cpu.unique()

"""También se aprecia que todas son frecuencias y además en la misma medida, por lo que parece ser que estábamos en lo cierto en base a que lo que ocurre con las 5 primeras muestras, se replica para el resto de ellas. Por tanto, se dividirá esta columna en 3, ya que será más eficiente a la hora de hacer análisis y gráficas. Para le valor numérico, como en casos anteriores, se eliminará la unidad de medida de las muestras poniendo esta en la cabecera"""

# Se realiza un splitado en base a 'GHz'
speed_cpu_split = speed_cpu.str.split('GHz', 0, expand=True)
# Se crea la serie sin medidas
speed_cpu_GHz = speed_cpu_split[0]

"""Finalmente, se crean estas columnas y se le da el tipo numérico a la última de ellas"""

# Introducimos las nuevas columnas
laptops_initial["CPU_Company"] = company_cpu
laptops_initial["CPU_Version"] = version_cpu
laptops_initial["CPU_Speed(GHz)"] = speed_cpu_GHz

# La transformamos a numéricas
laptops_initial["CPU_Speed(GHz)"] = pandas.to_numeric(laptops_initial["CPU_Speed(GHz)"])

# Eliminamos la columna inicial que tenia los datos de la CPU sin limpiar
del laptops_initial["Cpu"]

# Verificamos la correcta creación
laptops_initial.head(5)

"""Se aprecia que para la GPU parece seguir una estructura similar: VENDEDOR + VERSION. Siguiendo el mismo proceso que el anterior, nos encontraríamos:"""

# Se realiza un splitado en base al primer espacio, dejando en la primera parte la supuesta marca, mientras que en la segunda la versión
split_gpu_company_and_model = laptops_initial['Gpu'].str.split(' ', 1, expand=True)

# Almacenamos las marcas de Gpu
company_gpu = split_gpu_company_and_model[0]

# Comprobamos que efectivamente todas ellas corresponden a marcas
company_gpu.unique()

# Almacenamos las versiones de Gpu
version_gpu = split_gpu_company_and_model[1]

# Comprobamos que efectivamente todas ellas corresponden a marcas
version_gpu.unique()

"""Vemos como de esta manera facilitamos análisis posteriores, sobre todo en lo referente a la marca, en donde podremos estudiar si la misma influye en el precio. Añadiendo estas columnas a nuestro dataFrame"""

# Introducimos las nuevas columnas
laptops_initial["GPU_Company"] = company_gpu
laptops_initial["GPU_Version"] = version_gpu

# Eliminamos la columna inicial que tenia los datos de la GPU sin limpiar
del laptops_initial["Gpu"]

# Verificamos la correcta creación
laptops_initial.head(5)

"""Finalmente, para la resolución, la divideremos en dos partes. La primera de ellas, de extraer la resolución en píxeles por una expresión regex:"""

# Limpieza columna ScreenResolution (se dejan los valores que indican únicamente la resolución en píxeles)
pixels_ScreenResolution = laptops_initial.ScreenResolution.str.extract("(\d+x\d+)", expand=False)

# Se comprueba que no hay nulos (que para todos los valores se ha conseguido obtener los pixeles)
pixels_ScreenResolution.isnull().sum()

"""Para poder tratarlas numéricamente, se extraerá el la parte de los píxeles a lo alto y a lo ancho, separados en todas las medidas por una x"""

# Se realiza un splitado en base a la x
split_pixels_value = pixels_ScreenResolution.str.split('x', 0, expand=True)

# Se separan en ancho (width) y alto (high)
split_pixels_width = split_pixels_value[0]
split_pixels_high = split_pixels_value[1]

"""Una vez realizado este hecho, se extraerá también tipo de resolución que presenta el ordenador. Mientras que todas las muestras traían los pixeles, en este caso no ocurrirá lo mismo, ya que se ve en las 5 primeras muestras como en la segunda (id 1), no se presenta el tipo.



"""

# Se realiza un splitado en base al último espacio, ya que si tiene tipo, en la primera variable de este splitado estará el mismo
# y si no lo tiene, estarán los píxeles sacados anteriormente
split_pixels_by_last_space = laptops_initial.ScreenResolution.str.rsplit(' ', 1, expand=True)

# Se crea un objeto serie para almacenar los tipos
type_pixels = pandas.Series([])

# Comprobamos uno a uno si son igualas a los píxeles (no tiene tipo) o si no lo son (tiene el tipo)
for index, value in split_pixels_by_last_space[0].items():
  if (split_pixels_by_last_space[0][index] == pixels_ScreenResolution[index]):
    type_pixels[index]=np.NaN
  else:
    type_pixels[index]=split_pixels_by_last_space[0][index]

"""En este caso, como es de esperar aparecerán nulos que serán tratados en la sección Elementos vacíos II. Introducimos los nuevos valores en el dataframe """

# Introducimos las nuevas columnas
laptops_initial["ScreenResolution_Type"] = type_pixels
laptops_initial["ScreenResolution_Width"] = split_pixels_width
laptops_initial["ScreenResolution_High"] = split_pixels_high

# La transformamos a numéricas las dos últimas
laptops_initial["ScreenResolution_Width"] = pandas.to_numeric(laptops_initial["ScreenResolution_Width"])
laptops_initial["ScreenResolution_High"] = pandas.to_numeric(laptops_initial["ScreenResolution_High"])

# Eliminamos la columna inicial que tenia los datos de ScreenResolution sin limpiar
del laptops_initial["ScreenResolution"]

#Verificamos la correcta creación
laptops_initial.head(5)

"""## **Elementos vacíos II**

En este momento, se comprobará si se han introducido valores nulos en el dataFrame. Tan sólo sería esperado en el apartado de tipo de resolución de pantalla, en donde se ha visto que algunas no disponían de esta información
"""

laptops_initial.isnull().sum()

"""Efectivamente, se aprecia como para este caso se encuentran solo valores vacíos en la variable ScreenResolution_Type. Para su tratamiento, se realizará el método de indiciar los valores perdidos como la sustitución por una misma constante o etiqueta, en este caso, "Unknown". Esto se hace ya que falta un gran número de registro (un 33%), y el uso de otras técnicas como la moda harían que tuvieramos muchísimos datos 'no reales'. Otras técnicas basadas en modelos que lo predicen, tampoco las vemos oportunadas, ya que siendo un atributo como es el tipo de resolución, podría darse el caso de que dos ordenadores tuvieran las mismas características, y sin embargo, un tipo de resolución diferencia. Por tanto, aplicando esto al dataFrame:"""

# Cambiamos los valores nulos por el literal Unknow
laptops_initial["ScreenResolution_Type"].fillna("Unknown", inplace = True)

# Verificamos que no queda ningún valor nulo
laptops_initial.ScreenResolution_Type.isnull().sum()

"""Finalemente, para las variables con las que se han tratado, se ha visto como no hay valores que indiquen valores nulos o vacíos. Faltaría tratar ver los valores de las columnas que no se han tratado todavía: 
* Company	
* TypeName
* OpSys

Para comprobar que no hay valores nulos en estas variables, se imprimirán los diferentes valores que contiene cada una
"""

laptops_initial['Company'].unique()

laptops_initial['TypeName'].unique()

laptops_initial['OpSys'].unique()

"""Se puede apreciar como para todos los casos se encuentran valores no vacíos. Si bien es verdad que para sistema operativo encontramos 'No OS', que puede dar lugar a confusión. Sin embargo, este valor es válido, ya que algunos ordenadores pueden no tener sistema operativo integradado, y por tanto, posiblemente, abarate el coste del mismo

## **Valores extremos**

Se analizarán a continuación los valores extremos, empezando por la variable más relevante para el estudio: price_euros. En un primer momento, se realizará un análisis descriptivo de esta variable numérica, estudiando sus diferentes percentiles:
"""

# Percentiles y resumen estadístico de la columna de precios
laptops_initial["Price_euros"].describe()

"""Se aprecia como aparentemente, aunque el percentil 50 está en 977 y su 75 en 1487, la media es de 1123, lo que es indicio de que, tras el percentil 75 encontraremos algún valor más elevado, hasta llegar al máximo de 6099. Estudiaremos gráficamente en una recta la situación de estos datos"""

# Extraemos cada precio y el número de veces que salen
price_unique, counts = np.unique(laptops_initial['Price_euros'], return_counts=True)

# Representamos de azul los valores en donde se concentran la mayoría de las filas
# y en rojo las que se alejan de esta
sizes = counts*100
colors = ['blue']*len(price_unique)
colors[-1] = 'red'
plt.axhline(1, color='k', linestyle='--')
plt.scatter(price_unique, np.ones(len(price_unique)), s=sizes, color=colors)
plt.yticks([])
plt.show()

"""Se aprecia como hay una gran concentración de datos en la zona en torno a 1000 euros, y que va disminuyendo hasta llegar hasta los 3000, en donde los precios a partir de este punto empiezan a encontrarse más distantes hasta llegar a los 6000, que la función cataloga como punto alejado o outlier. Una forma más visual y algo más matemática de verlo sería por medio de los boxplots:"""

# Construción de boxplot
green_diamond = dict(markerfacecolor='g', marker='D')
fig, ax = plt.subplots()
ax.set_title('Boxplot por Precios')
ax.boxplot(laptops_initial['Price_euros'], flierprops=green_diamond, labels=["Precio"])

"""En este, se aprecia información como la que se ha comentado en párrafos anteriores. En esta gráfica se aprecia como, los puntos por encima de la T, son catalogados como extremos o anómalos. Estudiemos en el dataSet estos casos, para ver si efectivamente se tratan de outliers que debemos de eliminar o tratar al ser posibles valores incorrectos o los mismos son puntos de interés de nuestro análisis y han de estar presentes."""

# Ordenar los ordenadores por precio, e imprimir las columnas más caras
laptops_initial.sort_values('Price_euros').tail(10)

"""Se aprecia como sin duda, en la gran mayoría encontramos en esta sección ordenadores Gaming, los cuales necesitan una capacidad mayores que los ordenadores estándar para el día a día. Además, vemos por ejemplo que la diferencia entre los dos últimos reside en la diferencia de la memoria SSD, ya que las demás características son iguales. Por lo general, vemos que todos estos ordenadores, presentan una CPU y GPU muy potente, que consultando el precio de estas en el mercado, parece ser un elemento que aumentará bastante el precio. Para confirmar este hecho, vamos a realizar un resumen de estos ordenadores

"""

laptops_initial[laptops_initial['TypeName']=='Gaming'].describe()

"""Se aprecia que aunque encontramos también algún ordenador barato de estas características (699), la mayoría, incrementan todos los percentiles vistos anteriormente. Además, con el resto de característimas de rendimiento como la CPU, presentan valores altos en donde desde el percentil 50 toma el valor de 2.8, llegando a tomar 3.2 en su máximo.

Centrando ahora el estudio para el caso del peso:
"""

# Construción de boxplot
green_diamond = dict(markerfacecolor='g', marker='D')
fig, ax = plt.subplots()
ax.set_title('Boxplot por Precios')
ax.boxplot(laptops_initial['Weight(Kg)'], flierprops=green_diamond, labels=["Peso (Kg)"])

"""Se aprecia como se vuelven a tener varios registros que se alejan "por arriba" del conjunto de los demás datos. Para ello, volveremos a ver de que ordenadores se trata:"""

laptops_initial[laptops_initial['Weight(Kg)']>3.5]

"""Se aprecia como, de nuevo, vuelve a tratarse de ordenadores Gaming, los cuales parece ser que pesan más que los "normales", posiblemente, debido a que presentan componentes más pesados y pesados, siendo ordenadores centrados en el rendimiento y no tanto en la portabilidad. 
Para comprobar cómo estos ordenadores no son mayoría, se realizará un gráfico de barras en función del tipo de ordeandor:
"""

laptops_initial['TypeName'].value_counts().plot.bar()
plt.show()

"""Se aprecia como el tipo Notebook es el más presente en el dataSet, y por tanto, el que concetrará más ordeandores de las mismas características. El resto de ordenadores, aunque tienen diferentes funcionalidades, tienen un uso parecido entre ellos (notebook, ultrabook o 2 en 1), en donde sus diferencias en términos de características son pequeñas (que la pantalla sea táctil, que sea mas transportable...). Sin embargo, los gaming, suelen ser ordenadores más potentes, y habitualmente, más grandes, alejándose del resto de los ordenadores que hacen que se presente esta diferencia, y no por ello son datos inválidos. Serán simplemente, como el nombre del apartado indica, valores extremos.

Para el resto de variables numéricas, no tiene tanto sentido elaborar este estudio, ya que aunque son valores numéricos, son valores que no son continuos como tal, ya que las medidas dadas en GB son medidas específicas (8-16-64...), al igual que ocurre con los tamaños de pantalla, en donde suelen ser tamaños estandarizados (alto-ancho) y no llegan a ser continuos como tal. Además, estos valores se han visto en la preparación de datos anteriores, y no ha apreciado ningún valor fuera de lo común o medida desmesurada para un ordenador.

Una vez tenemos los datos limpios, los exportamos para añadirlos al git para por si en futuro alguien desea reutilizarlos:
"""

#Se exportan los datos limpios
laptops_initial.to_csv('laptops_procesados.csv')

"""# **4. Análisis de los datos**

La variable objetivo de este conjunto de datos es la variable de Price_euros, debido a que en apartados posteriores, realizaremos un contraste de medias y un modelo de regresión lineal.

En el contraste de medias, estudiaremos si se puede afirmar que es estadísticamente significativo que los productos de Apple tengan un coste más elevado que el resto de marcas. 

En la regresión lineal, se realizará un modelo que trate de predecir el precio de un dispositivo teniendo en cuenta sus características técnicas. Para ello, se hará uso de las variables independientes que muestren tener más impacto en el cálculo final del producto, realizando un análisis de sus distribuciones y correlación.

## **Resumen del dataSet**

En este apartado se realizará un resumen del dataSet con el que se va a trabajar, para que de un vistazo, se pueda extraer información útil del mismo.
"""

#Se realiza un pequeño resumen estadístico de las variables numéricas
 laptops_initial.describe()

"""Con lo que se ha visto hasta el momento, de esta tabla resumen lo primero que llama la atención es que se puede confirmar la afirmación mostrada anteriormente. Todos los valores presentan medias y percentiles menores que los que se han dado en los ordenadores gaming, menos en la memoria Flash, ya que los ordenadores de estas características priorizan los otros tipos de memoria, en especial la SSD. Por tanto, se aprecia como lo descrito anteriormente es cierto. 

Por otro lado, si ignoramos las variables de memoria, no se producen desviaciones estándar muy elevadas, excepto en el precio (que tiene sentido al tener ordenadores de diversas marcas, características) y en la resolución de pantalla, que tiene sentido también ya que son números más elevados y es lógico que de un notebook a un ordenador gamer se encuentre esta diferencia en la pantalla. 

En las memorias vemos como las desviaciones son mayores (teniendo en cuenta su unidad de medida, medias y límites), y esto es así por el hecho de que algunos ordenadores poseen un tipo de memoria y otros dos. Este hecho, por la decisión que se ha tomado antes, hace que si no se dispone de un tipo de memoria, se ponga el valor de 0 para ese caso, por lo que en una columna se pueden apreciar datos algo diversos por el hecho de que un ordenador tenga una memoria de un tipo pero de otra no. Debido a este hecho, y a que estas columnas se podrían llegar a considerar discretas (las memorias pueden ser de unos valores en concreto), se harán una representación gráfica de las mismas como con las categóricas.

Para las variables categóricas, siempre que no nos encontremos con una cantidad de posibles valores desorbitada, se realizarán gráficos de barra que permiten ver de una forma más visual el tipo de datos con los que se trabaja. En caso de que se encuentren muchas más categorías, se realizará una tabla de frecuencia relativa a esa variable, hecho que ocurre tanto para las versiones de GPU como de CPU como para los tipo de resolución . 

Por tanto, calculando las mismas:
"""

laptops_initial['Company'].value_counts().plot.bar()
plt.title("Distribución compañias")
plt.show()

laptops_initial['TypeName'].value_counts().plot.bar()
plt.title("Distribución tipo ordenador")
plt.show()

laptops_initial['OpSys'].value_counts().plot.bar()
plt.title("Distribución sistema operativo")
plt.show()

laptops_initial['MemorySSD(GB)'].value_counts().plot.bar()
plt.title("Distribución memoriaSSD")
plt.show()

laptops_initial['MemoryHDD(GB)'].value_counts().plot.bar()
plt.title("Distribución memoriaHDD")
plt.show()

laptops_initial['MemoryFlash(GB)'].value_counts().plot.bar()
plt.title("Distribución memoriaFlash")
plt.show()

laptops_initial['MemoryHybrid(GB)'].value_counts().plot.bar()
plt.title("Distribución memoriaHybrid")
plt.show()

laptops_initial['CPU_Company'].value_counts().plot.bar()
plt.title("Distribución Compañia CPU")
plt.show()

#laptops_initial['CPU_Version'].value_counts().plot.bar()
#plt.title("Distribución version CPU")
#plt.show()
# tabla de frecuencia
100 * laptops_initial['CPU_Version'].value_counts() / len(laptops_initial['CPU_Version'])

laptops_initial['GPU_Company'].value_counts().plot.bar()
plt.title("Distribución Compañia GPU")
plt.show()

#laptops_initial['GPU_Version'].value_counts().plot.bar()
#plt.title("Distribución version CPU")
#plt.show()
# tabla de frecuencia
100 * laptops_initial['GPU_Version'].value_counts() / len(laptops_initial['GPU_Version'])

#laptops_initial['ScreenResolution_Type'].value_counts().plot.bar()
#plt.title("Distribución tipo de resolución")
#plt.show()
# tabla de frecuencia
100 * laptops_initial['ScreenResolution_Type'].value_counts() / len(laptops_initial['ScreenResolution_Type'])

"""De las gráficas de barras se pueden extraer de primera mano varias conclusiones, como son:
- El dataset cuenta con gran cantidad de marcas de ordenadores diferentes, pero se observa una predominancia de Dell, Lenovo y HP. Posiblemente sea porque son los que más modelos tengan.
- Los ordenadores de tipo Notebok (al ser los más comunes en el día a día), son los que más porción ocupan del dataSet.
- El sistema operativo por excelencia es Windows 10, seguido muy de lejos por ordenadores libres y a continuación Linux.
- En el caso de contar memoria SSD (que más de la mitad cuentan con este tipo de memoria), la misma es de 256 GB. En el caso de tener HDD, es 1024GB, aunque predominan los ordenadores sin este tipo de memoria (habitualmente es porque los ordenadores, como se ha podido ver en la limpieza, es más habitualmente que posean memoria SSD que HDD al ser esta más rápida). Para los casos de flash e Hybrid, no la gran mayoría no disponen de esta memoria.
- Se puede afirmar, que Intel es la marca líder en el mercado en “darle vida al ordanador”. Esta marca es la líder en tanto en GPU como en CPU. En GPU, es seguida por Nvidia, sistema operativo presente en la gran mayoría de ordenadores Gaming como se puede apreciar en la tabla del apartado anterior. 


Respecto a estas tablas de frecuencias, se aprecia como ha gran cantidad de versiones tanto de CPU como de GPU. Estos son principalmente el core i5 7200U, que se lleva casi el 15% de ordenadores (este hecho puede ser debido a que es una versión de CPU de calidad baja válida para trabajos diarios), seguidos por CPU de calidad algo mayores al hablar de core i7. Para el cado de las GPU, predominan los HD Graphics 620 y 520, llevándose estas el 35% de los ordenadores. Finalmente, con respecto al tipo de resolución predomina la HD en su mayor parte, aunque en un 25% de las ocasiones, no se ha indicado el tipo de la misma como se ha analizado en el apartado de limpieza.

## **Contraste de medias**

Tal y como se ha indicado anteriormente, procedemos en este apartado a realizar un constraste de medias que nos indicará si los productos de Apple son significativamente más caros, de media, que el resto de productos de otras marcas. 

Para ello, separaremos primero el dataset en dos de tal forma que contengan respectivamente los precios de los productos de Apple y del resto de marcas.
"""

apple_prices = laptops_initial[laptops_initial["Company"] == "Apple"]["Price_euros"]
other_prices = laptops_initial[laptops_initial["Company"] != "Apple"]["Price_euros"]
print(apple_prices.head(5))
print(other_prices.head(5))

print(f'Precio medio de los productos de Apple: {round(st.mean(apple_prices), 2)}.')
print(f'Precio medio de los productos de otras marcas: {round(st.mean(other_prices), 2)}.')

"""Una vez separadas las muestras, estudiamos la normalidad de cada una de ellas:"""

# Histograma y gráfico de probabilidad normal de los precios de Apple:
sns.distplot(apple_prices, fit = norm);
fig = plt.figure()
res = stats.probplot(apple_prices, plot = plt)

# Prueba de Shapiro-Wilk
stat, p = shapiro(apple_prices)
print(f"Stat: {round(stat,3)}")
print(f"p-value: {round(p,3)}")

# Histograma y gráfico de probabilidad normal de los precios del resto de marcas:
sns.distplot(other_prices, fit = norm);
fig = plt.figure()
res = stats.probplot(other_prices, plot = plt)

# Prueba de Shapiro-Wilk
stat, p = shapiro(other_prices)
print(f"Stat: {round(stat,3)}")
print(f"p-value: {round(p,3)}")

"""En base a los resultados obtenidos, no es posible garantizar la normalidad en la distribución de las muestras de precios tanto de apple como de otras marcas dado que el test de Shapiro rechaza la hipótesis nula de normalidad. Asimismo, en el caso del subset de Apple, no sería posible tampoco aplicar el teorema del límite central al no superar las 30 muestras. 

En este sentido, para poder confirmar si podemos asumir homocedasticidad (igualdad de varianzas entre muestras) debemos aplicar el test de Fligner-Killeen (no paramétrico) al no poder suponer normalidad:
"""

# Fligner-Killeen test
fligner_test = stats.fligner(apple_prices, other_prices, center='median')
fligner_test

"""A raíz de los resultados (p value >> 0.05) no podemos descartar la hipótesis nula, por lo que se confirma la homocedasticidad. No obstante, debido a que no hemos podido afirmar que sigan distribuciones normales, no podremos aplicar un contraste de muestras paramétrico (t-Student), si no que tendremos que aplicar uno no paramétrico (Mann-Whitney):"""

# Mann-Whitney test
mannwhitneyu_test = stats.mannwhitneyu(apple_prices, other_prices, alternative="greater")
mannwhitneyu_test

"""En base a los resultados obtenidos del test de Mann-Whitney (p value << 0.05), podemos rechazar la hipótesis nula en favor de la hipótesis alternativa que, en este caso, correspondía con que el precio de los productos de Apple es superior de media que para el resto de marcas.

## **Estudio de variables numéricas**

Realizamos primero una matriz de correlación, que nos podrá indicar con qué 
variables tiene más relación el valor del precio:
"""

# Matriz de correlación:
corrmat = laptops_initial.corr()
f, ax = plt.subplots(figsize=(15, 8))
sns.heatmap(corrmat, annot=True, vmax=.2, square=True);

"""Tal y como se puede apreciar en el mapa de calor, la variable numérica con la que tiene más correlación el precio es con la cantidad de RAM que tenga el dispositivo, seguido de la cantidad de memoria SSD, la resolución de pantalla y la CPU que monte el dispositivo. Todas ellas, serán candidatas a formar parte del modelo de predicción del precio de un dispositivo.

Al contrario, las variables que menos influyen en la variabilidad de precio son el resto de tipos de almacenamiento (HDD, Flash e Híbrido), así como las pulgadas del portátil. El peso, pese a que influye en la valoración del portátil, lo hace de forma muy débil.

Dado que se identifica que la resolución de pantalla a lo ancho y alto están completamente correladas entre sí, se elimina la columna ScreenResolution_High para evitar redundancias.
"""

# Eliminación de la columna redundante ScreenResolution_High
del laptops_initial['ScreenResolution_High']

"""## **Estudio de variables categóricas**

A continuación, vamos a estudiar con más detalle algunas de las variables categóricas que pensamos pueden afectar más al rendimiento del modelo, para identificar visualmente si vemos conveniente que también formen parte del mismo:
"""

# Boxplot Company/Price_euros:
var = 'Company'
data = pandas.concat([laptops_initial['Price_euros'], laptops_initial[var]], axis=1)
f, ax = plt.subplots(figsize=(13, 6))
fig = sns.boxplot(x=var, y="Price_euros", data = data)
fig.axis(ymin=0, ymax=7000);
plt.xticks(rotation=90);

"""Respecto a las marcas, si bien hay algunas con rangos claramente superiores o inferiores, la mayoría de ellas mantienen una media de precios estable entorno a los 1000-2000€, rango que por otra parte se ha comprobado anteriormente como la media del dataset. En este sentido, no haremos uso de esta variable para el modelo."""

# Boxplot CPU_Company/Price_euros:
var = 'CPU_Company'
data = pandas.concat([laptops_initial['Price_euros'], laptops_initial[var]], axis=1)
f, ax = plt.subplots(figsize=(6, 6))
fig = sns.boxplot(x=var, y="Price_euros", data = data)
fig.axis(ymin=0, ymax=7000);
plt.xticks(rotation=90);

"""En este caso, se ve una clara diferencia de medias entre las CPU de Intel y las de AMD, que son las dos que abarcan la mayoría de dispositivos. Vemos conveniente por tanto disponer de dicha información en el modelo."""

# Boxplot GPU_Company/Price_euros:
var = 'GPU_Company'
data = pandas.concat([laptops_initial['Price_euros'], laptops_initial[var]], axis=1)
f, ax = plt.subplots(figsize=(6, 6))
fig = sns.boxplot(x=var, y="Price_euros", data = data)
fig.axis(ymin=0, ymax=7000);
plt.xticks(rotation=90);

"""En este caso, aunque no de forma tan clara como en el de las CPU, se pueden apreciar lo que serían 3 gamas de GPU. De más barata a más cara estarían: AMD, Intel y Nvidia. Por lo tanto, consideramos relevante añadirlo al modelo."""

# Boxplot TypeName/Price_euros:
var = 'TypeName'
data = pandas.concat([laptops_initial['Price_euros'], laptops_initial[var]], axis=1)
f, ax = plt.subplots(figsize=(6, 6))
fig = sns.boxplot(x=var, y="Price_euros", data = data)
fig.axis(ymin=0, ymax=7000);
plt.xticks(rotation=90);

"""En tipos de dispositivos, si bien hay alguna categoría que destaca (como Gaming), el resto de dispositivos parece que abarcan una franja amplia de precios, con lo que no se ve recomendable incorporarla al modelo.

A continuación procedemos a codificar las variables categóricas como numéricas mediante el método de one-hot para su inclusión en el modelo que generaremos a posteriori. Dado que desgranamos una variable categórica en tantas variables como categorias tenga (k), siempre podremos eliminar una de estas variables resultantes para mejorar el rendimiento del modelo y evitar redundancias.
"""

# One-Hot de CPU_Company
dummies = pandas.get_dummies(laptops_initial['CPU_Company'], drop_first = True)
dummies = dummies.rename(columns={"Intel": "CPU_Intel", "Samsung": "CPU_Samsung"})
laptops_initial = laptops_initial.join(dummies)
laptops_initial

# One-Hot de GPU_Company
dummies = pandas.get_dummies(laptops_initial['GPU_Company'], drop_first = True)
dummies = dummies.rename(columns={"ARM": "GPU_ARM", "Intel": "GPU_Intel", "Nvidia": "GPU_Nvidia"})
laptops_initial = laptops_initial.join(dummies)
laptops_initial

"""Para tratar de ver el efecto real de estas variables, ejecutaremos de nuevo la matriz de correlación y veremos si la relación entre las variables que hemos pasado a numéricas y el precio es buena para incluirlas en el modelo."""

# Matriz de correlación:
corrmat = laptops_initial.corr()
f, ax = plt.subplots(figsize=(25, 12))
sns.heatmap(corrmat, annot=True, vmax=.2, square=True);

"""A partir de los resultados, vemos que la única variables que podría ser relevante para el modelo es la de GPU_Nvidia, al tener una correlación con el precio de 0.35. También podría llegar a ser candidata CPU_Intel, al haber obtenido un 0.18 de correlación.

## **Creación y evaluación del modelo**
"""

# Generación del dataset para cargar en el modelo con las columnas seleccionadas
laptops_finished = laptops_initial[["Ram(GB)", "Weight(Kg)", "MemorySSD(GB)", "CPU_Speed(GHz)", "ScreenResolution_Width", "GPU_Nvidia", "Price_euros"]]

# Normalización de los valores del dataset
scaler = MinMaxScaler(feature_range=(0,1))
ind_cols = ["Ram(GB)", "Weight(Kg)", "MemorySSD(GB)", "CPU_Speed(GHz)", "ScreenResolution_Width", "GPU_Nvidia", "Price_euros"]
final_data = laptops_finished.values
laptops_finished = pandas.DataFrame(scaler.fit_transform(final_data), columns=ind_cols)
laptops_finished.head(5)

"""A continuación, procederemos a dividir el dataset en un subset de entrenamiento y otro de test para evaluar el rendimiento del modelo."""

# Hacemos uso de la librería sklearn para dividir en conjunto de train y test
train, test = train_test_split(laptops_finished, test_size = 0.30)

# Preparación de las variables de train y test separando variables dependientes e independientes
x_train = train.iloc[:, :-1]
y_train = train.iloc[:, -1]
x_test = test.iloc[:, :-1]
y_test = test.iloc[:, -1]

# Creación del modelo de regresión lineal
model = LinearRegression().fit(x_train, y_train)
r_sq = model.score(x_train, y_train)
print('Coefficient of determination:', round(r_sq, 2), "\n")

# Ejecución del modelo para predecir precios sobre el df de test
y_pred = model.predict(x_test)

# Inversión de la normalización para tener los precios en la escala correcta
# Se debe disponer de un dataframe del mismo tamaño que el normalizado
x_test = x_test.reset_index(drop=True)
interm_df = pandas.concat([x_test, pandas.Series(y_pred)], axis=1)

# Aplicación de la inversión de la normalización
y_pred = scaler.inverse_transform(interm_df)[:, -1]
y_test = scaler.inverse_transform(test)[:, -1]

# Generación de un dataframe para presentar los resultados reales y predichos
pred_price = pandas.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(pred_price)

"""Los resultados de esta regresión no dan un resultado excesivamente alto respecto a la precisión del modelo, ya que esta se queda en un coeficiente de determinación de 0.7, lo que indica que el 70% de la variabilidad de la variable dependiente está explicada por las variables independientes. No obstante, puede ser suficiente para identificar si un precio de mercado está por encima o por debajo de lo esperado por sus especificaciones."""